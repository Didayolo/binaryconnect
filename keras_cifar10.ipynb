{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary neural network\n",
    "\n",
    "On veux tester la binarisation des poids du réseau neuronal durant l'apprentissage.\n",
    "On le test sur CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use GPU for Theano, comment to use CPU instead of GPU\n",
    "# Tensorflow uses GPU by default\n",
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "np.random.seed(2017) \n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To visiualize models\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# If using tensorflow, set image dimensions order\n",
    "from keras import backend as K\n",
    "if K.backend()=='tensorflow':\n",
    "    K.set_image_dim_ordering(\"th\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "# Evolution of learning rate\n",
    "\n",
    "from binary_ops import binary_tanh as binary_tanh_op\n",
    "from binary_layers import BinaryDense, BinaryConv2D, Clip\n",
    "\n",
    "class DropoutNoScale(Dropout):\n",
    "    '''Keras Dropout does scale the input in training phase, which is undesirable here.\n",
    "    '''\n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(inputs, self.rate, noise_shape,\n",
    "                                 seed=self.seed) * (1 - self.rate)\n",
    "            return K.in_train_phase(dropped_inputs, inputs,\n",
    "                                    training=training)\n",
    "        return inputs\n",
    "\n",
    "def binary_tanh(x):\n",
    "    return binary_tanh_op(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processings : \n",
    "\n",
    "- Global contrast normalization\n",
    "- ZCA whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "from __future__ import division\n",
    "\n",
    "def normalization(X, up):\n",
    "    \"\"\" Normalize X beetween its minimum and up \"\"\"\n",
    "    return (X - np.min(X)) * up / (np.max(X) - np.min(X))\n",
    "\n",
    "def global_contrast_normalization(Y):\n",
    "    \"\"\" Apply GCN on image Y \"\"\"\n",
    "    X = Y.copy()\n",
    "    average = np.mean(X)\n",
    "    dev_stand = np.std(X)\n",
    "    \n",
    "    for i in range(0, len(X)):\n",
    "        av = X[i] - average\n",
    "        dev = av / dev_stand\n",
    "        X[i] = normalization(dev, 254.0)\n",
    "    return X\n",
    "    \n",
    "def zca_whiten(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Function to compute ZCA whitening matrix (aka Mahalanobis whitening).\n",
    "    INPUT:  X_train and y_train\n",
    "    OUTPUT: Whiten X_train\n",
    "    \"\"\"\n",
    "    # Attention, fonction sale\n",
    "    datagen = ImageDataGenerator(zca_whitening=True)\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied)\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    flow = datagen.flow(x_train, y_train, batch_size=1, shuffle=False)\n",
    "\n",
    "    whiten_images = []\n",
    "\n",
    "    for _ in range(x_train.shape[0]):\n",
    "        x, y = flow.next()\n",
    "        whiten_images.append(x[0])\n",
    "        \n",
    "    return whiten_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load outside CIFAR-10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#Choisit le test set\n",
    "#Source = blurred, normal, etc..\n",
    "classes = ['airplanes', 'automobiles', 'birds', 'cats', 'deers', 'dogs', 'frogs', 'horses', 'ships', 'trucks']\n",
    "\n",
    "def pick_test_set(source):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for dossier, sous_dossiers, fichiers in os.walk(\"Images/\" + source):\n",
    "        split = dossier.split(\"/\")\n",
    "        classe = \"\"\n",
    "        if (len(split) >=3):\n",
    "            classe = split[2]\n",
    "            #print(\"Processing \" + classe + \"...\")\n",
    "        \n",
    "        #Pour l'instant on ne garde que les vraies classes\n",
    "        if (classe != \"birdogs\" and classe != \"birdsdogs\" and classe != \"catsdogs\" and classe != \"others\"):\n",
    "            print(\"Processing \" + classe + \"...\")\n",
    "            \n",
    "            for fichier in fichiers: #Pour chaque image\n",
    "\n",
    "                path = dossier + \"/\"+ fichier\n",
    "                img = cv2.imread(path)\n",
    "                img = img[:,:,::-1] # BGR to RGB\n",
    "                # print(img.shape)\n",
    "                # TODO\n",
    "                image = np.transpose(np.reshape(img,(32,32,3)), (2,0,1)) # To CIFAR format\n",
    "                \n",
    "                labels.append([classes.index(classe)])\n",
    "                images.append(image)\n",
    "                \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ...\n",
      "Processing ships...\n",
      "Processing deers...\n",
      "Processing frogs...\n",
      "Processing trucks...\n",
      "Processing cats...\n",
      "Processing dogs...\n",
      "Processing birds...\n",
      "Processing airplanes...\n",
      "Processing automobiles...\n",
      "Processing horses...\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()\n",
    "\n",
    "# USE OUR TEST DATA\n",
    "test_features, test_labels = pick_test_set(\"Resized\")\n",
    "\n",
    "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
    "num_test, _, _, _ =  test_features.shape\n",
    "num_classes = len(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global contrast normalization...\n"
     ]
    }
   ],
   "source": [
    "train_features = train_features.astype('float32')/255\n",
    "test_features = test_features.astype('float32')/255\n",
    "# convert class labels to binary class labels\n",
    "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = np_utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Mieux vaut utiliser la fonction zca_whiten pour quelques exemples dans le rapport, mais utiliser les flows pour passer les données\n",
    "# pré traiter à Keras direct lors de l'apprentissage -> moins d'embrouilles. Limite dans la fonction train\n",
    "# voir train_processing\n",
    "\n",
    "# ZCA whitening\n",
    "#print(\"ZCA whitening...\")\n",
    "#train_features = zca_whiten(train_features, train_labels) # labels facultatifs.. Mais 5h du mat'\n",
    "#test_features = zca_whiten(test_features, train_labels)\n",
    "\n",
    "# Global Contrast Normalization\n",
    "print(\"Global contrast normalization...\")\n",
    "for i in range(0, len(train_features)):\n",
    "    train_features[i] = global_contrast_normalization(train_features[i])\n",
    "    \n",
    "for i in range(0, len(test_features)):\n",
    "    test_features[i] = global_contrast_normalization(test_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Examples from Each Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-a049f05cc591>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-a049f05cc591>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    show_examples(train_features[], train_labels[])\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def show_examples(features, labels):\n",
    "\n",
    "    class_names = ['airplane','automobile','bird','cat','deer',\n",
    "                   'dog','frog','horse','ship','truck']\n",
    "    fig = plt.figure(figsize=(8,3))\n",
    "    for i in range(num_classes):\n",
    "        ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "        idx = np.where(labels[:]==i)[0]\n",
    "        features_idx = features[idx,::]\n",
    "        img_num = np.random.randint(features_idx.shape[0])\n",
    "        im = np.transpose(features_idx[img_num,::], (1, 2, 0))\n",
    "        ax.set_title(class_names[i])\n",
    "        plt.imshow(im)\n",
    "    plt.show()\n",
    "    \n",
    "show_examples(train_features, train_labels)\n",
    "show_examples(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "\n",
    "img = wimages[i]\n",
    "#img = normalization(img, 1.0)\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = train_features[i]\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to plot model accuracy and loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtion to compute test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(test_x, test_y, model):\n",
    "    result = model.predict(test_x)\n",
    "    predicted_class = np.argmax(result, axis=1)\n",
    "    true_class = np.argmax(test_y, axis=1)\n",
    "    num_correct = np.sum(predicted_class == true_class) \n",
    "    accuracy = float(num_correct)/result.shape[0]\n",
    "    return (accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, callbacks=None, nb_epoch=10, batch_size=128, verbose=1, lr_start=1e-3, lr_end=1e-4):\n",
    "    # Train the model\n",
    "    \n",
    "    lr_decay = (lr_end / lr_start)**(1. / nb_epoch)\n",
    "    \n",
    "    if callbacks==None:\n",
    "        lr_scheduler = LearningRateScheduler(lambda e: lr_start * lr_decay ** e)\n",
    "        callbacks = [lr_scheduler]\n",
    "    \n",
    "    start = time.time()\n",
    "    model_info = model.fit(train_features, train_labels, \n",
    "                           batch_size=batch_size, nb_epoch=nb_epoch, \n",
    "                           validation_data = (test_features, test_labels), \n",
    "                           verbose=verbose, callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    # plot model history\n",
    "    plot_model_history(model_info)\n",
    "    print \"Model took %0.2f seconds to train\"%(end - start)\n",
    "    # compute test accuracy\n",
    "    print \"Accuracy on test data is: %0.2f\"%accuracy(test_features, test_labels, model)\n",
    "    \n",
    "    \n",
    "def train_processing(model, callbacks=None, nb_epoch=10, batch_size=128, verbose=1, lr_start=1e-3, lr_end=1e-4, zca_whitening=True):\n",
    "    # Train the model\n",
    "    \n",
    "    lr_decay = (lr_end / lr_start)**(1. / nb_epoch)\n",
    "    \n",
    "    if callbacks==None:\n",
    "        lr_scheduler = LearningRateScheduler(lambda e: lr_start * lr_decay ** e)\n",
    "        callbacks = [lr_scheduler]\n",
    "    \n",
    "    print(\"Pre-processing data...\")\n",
    "    datagen = ImageDataGenerator(zca_whitening=zca_whitening)\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied)\n",
    "    datagen.fit(train_features)\n",
    "    \n",
    "    start = time.time()\n",
    "    model_info = # fits the model on batches with real-time data augmentation:\n",
    "    model.fit_generator(datagen.flow(train_features, train_labels, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(train_features) / nb_epoch, epochs=nb_epoch,\n",
    "                        verbose=verbose, callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    # plot model history\n",
    "    plot_model_history(model_info)\n",
    "    print \"Model took %0.2f seconds to train\"%(end - start)\n",
    "    # compute test accuracy\n",
    "    print \"Accuracy on test data is: %0.2f\"%accuracy(test_features, test_labels, model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_weights(model, h5_file):\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "def load_weights(model, h5_file):\n",
    "    # load weights into new model\n",
    "    model.load_weights(h5_file)\n",
    "    print(\"Loaded model from disk\")\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple network, without binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_unit = 100\n",
    "num_hidden = 3\n",
    "lr_start = 1e-3\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Simple layer\n",
    "model.add(Flatten(input_shape=(3, 32, 32)))\n",
    "for _ in range(num_hidden):\n",
    "    model.add(Dense(num_unit, activation='tanh'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_classes, activation='softmax')) # 10SVM (L2-SVM) coming soon\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='squared_hinge', optimizer=Adam(lr=lr_start), metrics=['acc'])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "#train(model)\n",
    "\n",
    "# plot_model(model, to_file='simple_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper's architecture, without binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h5_file = \"paper_nb.h5\"\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# (2 * 128C3)\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same', input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "\n",
    "# MP2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25)) # test\n",
    "\n",
    "# (2 * 256C3)\n",
    "model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "\n",
    "# MP2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# (2 * 512C3)\n",
    "model.add(Convolution2D(512, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "\n",
    "# MP2\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten()) # Flat before FC\n",
    "\n",
    "# (2 * 1024FC)\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(num_classes, activation='linear', W_regularizer=l2(0.01))) # 10SVM (L2-SVM) coming soon\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "#train(model)\n",
    "\n",
    "# Load the model\n",
    "# model = load(model, h5_file)\n",
    "\n",
    "# Save the model\n",
    "#save(model, h5_file)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple network, with binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deterministic = True # deterministic or stochastic binarization\n",
    "\n",
    "H = 'Glorot'\n",
    "\n",
    "# network\n",
    "num_unit = 100\n",
    "num_hidden = 3\n",
    "use_bias = False\n",
    "\n",
    "# BN\n",
    "epsilon = 1e-6\n",
    "momentum = 0.9\n",
    "\n",
    "# dropout\n",
    "drop_in = 0.2\n",
    "drop_hidden = 0.5\n",
    "\n",
    "# lr\n",
    "lr_start = 1e-3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(DropoutNoScale(drop_in, input_shape=(3, 32, 32), name='drop0'))\n",
    "model.add(Flatten())\n",
    "for i in range(num_hidden):\n",
    "    model.add(BinaryDense(num_unit, H=H, use_bias=use_bias, # deterministic=deterministic\n",
    "              name='dense{}'.format(i+1)))\n",
    "    model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn{}'.format(i+1)))\n",
    "    model.add(Activation(binary_tanh, name='act{}'.format(i+1)))\n",
    "    model.add(DropoutNoScale(drop_hidden, name='drop{}'.format(i+1)))\n",
    "# Output layer\n",
    "model.add(BinaryDense(10, H=H, use_bias=use_bias, # deterministic=deterministic\n",
    "          name='dense'))\n",
    "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn'))\n",
    "\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='squared_hinge', optimizer=Adam(lr=lr_start), metrics=['acc'])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "#train(model, lr_start=lr_start)\n",
    "\n",
    "model = load_weights(model, \"test.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary weights complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "#model = Sequential()\n",
    "\n",
    "\"\"\"\n",
    "model.add(BinaryConv2D(128, deterministic=deterministic, kernel_size=kernel_size, input_shape=(channels, img_rows, img_cols),\n",
    "                       data_format='channels_first',\n",
    "                       H=H, kernel_lr_multiplier=kernel_lr_multiplier, \n",
    "                       padding='same', use_bias=use_bias, name='conv1'))\n",
    "model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, axis=1, name='bn1'))\n",
    "\"\"\"\n",
    "\n",
    "# Load the model\n",
    "# model = load(model, h5_file)\n",
    "\n",
    "# Save the model\n",
    "#save(model, yaml_file, h5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(zoom_range=0.2, \n",
    "                             horizontal_flip=True)\n",
    "\n",
    "\n",
    "# train the model\n",
    "start = time.time()\n",
    "# Train the model\n",
    "model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128),\n",
    "                                 samples_per_epoch = train_features.shape[0], nb_epoch = 200, \n",
    "                                 validation_data = (test_features, test_labels), verbose=0)\n",
    "end = time.time()\n",
    "print \"Model took %0.2f seconds to train\"%(end - start)\n",
    "# plot model history\n",
    "plot_model_history(model_info)\n",
    "# compute test accuracy\n",
    "print \"Accuracy on test data is: %0.2f\"%accuracy(test_features, test_labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
